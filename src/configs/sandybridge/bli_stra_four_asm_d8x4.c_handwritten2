#include "blis.h"

void bli_dstra_four_asm_8x4(
                           dim_t k,
                           double* restrict alpha,
                           double* restrict a,
                           double* restrict b,
                           double* restrict beta,
                           unsigned N,
                           double **c_list,
                           double *coeff_list,
                           inc_t rs_c, inc_t cs_c,
                           auxinfo_t* restrict data,
                           cntx_t*    restrict cntx
                          )
{
    void*   b_next = bli_auxinfo_next_b( data );

    uint64_t k_iter = k / 4;
    uint64_t k_left = k % 4;
    double *coeff0 = &coeff_list[0], *coeff1 = &coeff_list[1], *coeff2 = &coeff_list[2], *coeff3 = &coeff_list[3];
    double *c0 = c_list[0], *c1 = c_list[1], *c2 = c_list[2], *c3 = c_list[3];

	__asm__ volatile
	(
	"                                            \n\t"
	"                                            \n\t"
    "movq                %[a], %%rax             \n\t" // load address of a.              ( v )
    "movq                %[b], %%rbx             \n\t" // load address of b.              ( v )
    "movq                %[b_next], %%r15        \n\t" // load address of b_next.         ( v )
    "addq          $-4 * 64, %%r15               \n\t" //                                 ( ? )
    "                                            \n\t"
    "vmovapd   0 * 32(%%rax), %%ymm0             \n\t" // initialize loop by pre-loading
    "vmovapd   0 * 32(%%rbx), %%ymm2             \n\t" // elements of a and b.
    "vpermilpd  $0x5, %%ymm2, %%ymm3             \n\t"
    "                                            \n\t"
    "                                            \n\t"
    "movq                %[cs_c], %%rdi          \n\t" // load cs_c
    "leaq        (,%%rdi,8), %%rdi               \n\t" // cs_c * sizeof(double)
    "movq            %[c0], %%rcx               \n\t" // load address of c0
    "leaq   (%%rcx,%%rdi,2), %%r8               \n\t" // load address of c0 + 2 * ldc;
    "prefetcht0   3 * 8(%%rcx)                   \n\t" // prefetch c0 + 0 * ldc
    "prefetcht0   3 * 8(%%rcx,%%rdi)             \n\t" // prefetch c0 + 1 * ldc
    "prefetcht0   3 * 8(%%r8)                   \n\t" // prefetch c0 + 2 * ldc
    "prefetcht0   3 * 8(%%r8,%%rdi)             \n\t" // prefetch c0 + 3 * ldc
    "movq            %[c1], %%rdx               \n\t" // load address of c1
    "leaq   (%%rdx,%%rdi,2), %%r9               \n\t" // load address of c1 + 2 * ldc;
    "prefetcht0   3 * 8(%%rdx)                   \n\t" // prefetch c1 + 0 * ldc
    "prefetcht0   3 * 8(%%rdx,%%rdi)             \n\t" // prefetch c1 + 1 * ldc
    "prefetcht0   3 * 8(%%r9)                   \n\t" // prefetch c1 + 2 * ldc
    "prefetcht0   3 * 8(%%r9,%%rdi)             \n\t" // prefetch c1 + 3 * ldc
    "movq            %[c2], %%r10               \n\t" // load address of c2
    "leaq   (%%r10,%%rdi,2), %%r11               \n\t" // load address of c2 + 2 * ldc;
    "prefetcht0   3 * 8(%%r10)                   \n\t" // prefetch c2 + 0 * ldc
    "prefetcht0   3 * 8(%%r10,%%rdi)             \n\t" // prefetch c2 + 1 * ldc
    "prefetcht0   3 * 8(%%r11)                   \n\t" // prefetch c2 + 2 * ldc
    "prefetcht0   3 * 8(%%r11,%%rdi)             \n\t" // prefetch c2 + 3 * ldc
    "movq            %[c3], %%r14               \n\t" // load address of c3
    "leaq   (%%r14,%%rdi,2), %%r15               \n\t" // load address of c3 + 2 * ldc;
    "prefetcht0   3 * 8(%%r14)                   \n\t" // prefetch c3 + 0 * ldc
    "prefetcht0   3 * 8(%%r14,%%rdi)             \n\t" // prefetch c3 + 1 * ldc
    "prefetcht0   3 * 8(%%r15)                   \n\t" // prefetch c3 + 2 * ldc
    "prefetcht0   3 * 8(%%r15,%%rdi)             \n\t" // prefetch c3 + 3 * ldc
    "                                            \n\t"
	"vxorpd    %%ymm8,  %%ymm8,  %%ymm8          \n\t" // set ymm8 to 0                   ( v )
	"vxorpd    %%ymm9,  %%ymm9,  %%ymm9          \n\t"
	"vxorpd    %%ymm10, %%ymm10, %%ymm10         \n\t"
	"vxorpd    %%ymm11, %%ymm11, %%ymm11         \n\t"
	"vxorpd    %%ymm12, %%ymm12, %%ymm12         \n\t"
	"vxorpd    %%ymm13, %%ymm13, %%ymm13         \n\t"
	"vxorpd    %%ymm14, %%ymm14, %%ymm14         \n\t"
	"vxorpd    %%ymm15, %%ymm15, %%ymm15         \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"movq      %[k_iter], %%rsi                  \n\t" // i = k_iter;                     ( v )
	"testq  %%rsi, %%rsi                         \n\t" // check i via logical AND.        ( v )
	"je     .DCONSIDKLEFT                        \n\t" // if i == 0, jump to code that    ( v )
	"                                            \n\t" // contains the k_left loop.
	"                                            \n\t"
	"                                            \n\t"
	".DLOOPKITER:                                \n\t" // MAIN LOOP
	"                                            \n\t"
	"addq         $4 * 4 * 8,  %%r15             \n\t" // b_next += 4*4 (unroll x nr)     ( v )
	"                                            \n\t"
	"                                            \n\t" // iteration 0
	"vmovapd   1 * 32(%%rax),  %%ymm1            \n\t" // preload a47 for iter 0
	"vmulpd           %%ymm0,  %%ymm2,  %%ymm6   \n\t" // ymm6 ( c_tmp0 ) = ymm0 ( a03 ) * ymm2( b0 )
	"vperm2f128 $0x3, %%ymm2,  %%ymm2,  %%ymm4   \n\t" // ymm4 ( b0x3_0 )
	"vmulpd           %%ymm0,  %%ymm3,  %%ymm7   \n\t" // ymm7 ( c_tmp1 ) = ymm0 ( a03 ) * ymm3( b0x5 )
	"vperm2f128 $0x3, %%ymm3,  %%ymm3,  %%ymm5   \n\t" // ymm5 ( b0x3_1 )
	"vaddpd           %%ymm15, %%ymm6,  %%ymm15  \n\t" // ymm15 ( c_03_0 ) += ymm6( c_tmp0 )
	"vaddpd           %%ymm13, %%ymm7,  %%ymm13  \n\t" // ymm13 ( c_03_1 ) += ymm7( c_tmp1 )
	"                                            \n\t"
	"prefetcht0  16 * 32(%%rax)                  \n\t" // prefetch a03 for iter 1
	"vmulpd           %%ymm1,  %%ymm2,  %%ymm6   \n\t"
	"vmovapd   1 * 32(%%rbx),  %%ymm2            \n\t" // preload b for iter 1
	"vmulpd           %%ymm1,  %%ymm3,  %%ymm7   \n\t"
	"vpermilpd  $0x5, %%ymm2,  %%ymm3            \n\t"
	"vaddpd           %%ymm14, %%ymm6,  %%ymm14  \n\t"
	"vaddpd           %%ymm12, %%ymm7,  %%ymm12  \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm0,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm0,  %%ymm5,  %%ymm7   \n\t"
	"vmovapd   2 * 32(%%rax),  %%ymm0            \n\t" // preload a03 for iter 1
	"vaddpd           %%ymm11, %%ymm6,  %%ymm11  \n\t"
	"vaddpd           %%ymm9,  %%ymm7,  %%ymm9   \n\t"
	"prefetcht0   0 * 32(%%r15)                  \n\t" // prefetch b_next[0*4]
	"                                            \n\t"
	"vmulpd           %%ymm1,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm1,  %%ymm5,  %%ymm7   \n\t"
	"vaddpd           %%ymm10, %%ymm6,  %%ymm10  \n\t"
	"vaddpd           %%ymm8,  %%ymm7,  %%ymm8   \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t" // iteration 1
	"vmovapd   3 * 32(%%rax),  %%ymm1            \n\t" // preload a47 for iter 1
	"vmulpd           %%ymm0,  %%ymm2,  %%ymm6   \n\t"
	"vperm2f128 $0x3, %%ymm2,  %%ymm2,  %%ymm4   \n\t"
	"vmulpd           %%ymm0,  %%ymm3,  %%ymm7   \n\t"
	"vperm2f128 $0x3, %%ymm3,  %%ymm3,  %%ymm5   \n\t"
	"vaddpd           %%ymm15, %%ymm6,  %%ymm15  \n\t"
	"vaddpd           %%ymm13, %%ymm7,  %%ymm13  \n\t"
	"                                            \n\t"
	"prefetcht0  18 * 32(%%rax)                  \n\t" // prefetch a for iter 9  ( ? )
	"vmulpd           %%ymm1,  %%ymm2,  %%ymm6   \n\t"
	"vmovapd   2 * 32(%%rbx),  %%ymm2            \n\t" // preload b for iter 2 
	"vmulpd           %%ymm1,  %%ymm3,  %%ymm7   \n\t"
	"vpermilpd  $0x5, %%ymm2,  %%ymm3            \n\t"
	"vaddpd           %%ymm14, %%ymm6,  %%ymm14  \n\t"
	"vaddpd           %%ymm12, %%ymm7,  %%ymm12  \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm0,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm0,  %%ymm5,  %%ymm7   \n\t"
	"vmovapd   4 * 32(%%rax),  %%ymm0            \n\t" // preload a03 for iter 2
	"vaddpd           %%ymm11, %%ymm6,  %%ymm11  \n\t"
	"vaddpd           %%ymm9,  %%ymm7,  %%ymm9   \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm1,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm1,  %%ymm5,  %%ymm7   \n\t"
	"vaddpd           %%ymm10, %%ymm6,  %%ymm10  \n\t"
	"vaddpd           %%ymm8,  %%ymm7,  %%ymm8   \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t" // iteration 2
	"vmovapd   5 * 32(%%rax),  %%ymm1            \n\t" // preload a47 for iter 2
	"vmulpd           %%ymm0,  %%ymm2,  %%ymm6   \n\t"
	"vperm2f128 $0x3, %%ymm2,  %%ymm2,  %%ymm4   \n\t"
	"vmulpd           %%ymm0,  %%ymm3,  %%ymm7   \n\t"
	"vperm2f128 $0x3, %%ymm3,  %%ymm3,  %%ymm5   \n\t"
	"vaddpd           %%ymm15, %%ymm6,  %%ymm15  \n\t"
	"vaddpd           %%ymm13, %%ymm7,  %%ymm13  \n\t"
	"                                            \n\t"
	"prefetcht0  20 * 32(%%rax)                  \n\t" // prefetch a for iter 10 ( ? )
	"vmulpd           %%ymm1,  %%ymm2,  %%ymm6   \n\t"
	"vmovapd   3 * 32(%%rbx),  %%ymm2            \n\t" // preload b for iter 3
	"addq         $4 * 4 * 8,  %%rbx             \n\t" // b += 4*4 (unroll x nr)
	"vmulpd           %%ymm1,  %%ymm3,  %%ymm7   \n\t"
	"vpermilpd  $0x5, %%ymm2,  %%ymm3            \n\t"
	"vaddpd           %%ymm14, %%ymm6,  %%ymm14  \n\t"
	"vaddpd           %%ymm12, %%ymm7,  %%ymm12  \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm0,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm0,  %%ymm5,  %%ymm7   \n\t"
	"vmovapd   6 * 32(%%rax),  %%ymm0            \n\t" // preload a03 for iter 3
	"vaddpd           %%ymm11, %%ymm6,  %%ymm11  \n\t"
	"vaddpd           %%ymm9,  %%ymm7,  %%ymm9   \n\t"
	"prefetcht0   2 * 32(%%r15)                  \n\t" // prefetch b_next[2*4]
	"                                            \n\t"
	"vmulpd           %%ymm1,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm1,  %%ymm5,  %%ymm7   \n\t"
	"vaddpd           %%ymm10, %%ymm6,  %%ymm10  \n\t"
	"vaddpd           %%ymm8,  %%ymm7,  %%ymm8   \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t" // iteration 3
	"vmovapd   7 * 32(%%rax),  %%ymm1            \n\t" // preload a47 for iter 3
	"addq         $4 * 8 * 8,  %%rax             \n\t" // a += 4*8 (unroll x mr)
	"vmulpd           %%ymm0,  %%ymm2,  %%ymm6   \n\t"
	"vperm2f128 $0x3, %%ymm2,  %%ymm2,  %%ymm4   \n\t"
	"vmulpd           %%ymm0,  %%ymm3,  %%ymm7   \n\t"
	"vperm2f128 $0x3, %%ymm3,  %%ymm3,  %%ymm5   \n\t"
	"vaddpd           %%ymm15, %%ymm6,  %%ymm15  \n\t"
	"vaddpd           %%ymm13, %%ymm7,  %%ymm13  \n\t"
	"                                            \n\t"
	"prefetcht0  14 * 32(%%rax)                  \n\t" // prefetch a for iter 11 ( ? )
	"vmulpd           %%ymm1,  %%ymm2,  %%ymm6   \n\t"
	"vmovapd   0 * 32(%%rbx),  %%ymm2            \n\t" // preload b for iter 4
	"vmulpd           %%ymm1,  %%ymm3,  %%ymm7   \n\t"
	"vpermilpd  $0x5, %%ymm2,  %%ymm3            \n\t"
	"vaddpd           %%ymm14, %%ymm6,  %%ymm14  \n\t"
	"vaddpd           %%ymm12, %%ymm7,  %%ymm12  \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm0,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm0,  %%ymm5,  %%ymm7   \n\t"
	"vmovapd   0 * 32(%%rax),  %%ymm0            \n\t" // preload a03 for iter 4
	"vaddpd           %%ymm11, %%ymm6,  %%ymm11  \n\t"
	"vaddpd           %%ymm9,  %%ymm7,  %%ymm9   \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm1,  %%ymm4,  %%ymm6   \n\t"
	"vmulpd           %%ymm1,  %%ymm5,  %%ymm7   \n\t"
	"vaddpd           %%ymm10, %%ymm6,  %%ymm10  \n\t"
	"vaddpd           %%ymm8,  %%ymm7,  %%ymm8   \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"decq   %%rsi                                \n\t" // i -= 1;
	"jne    .DLOOPKITER                          \n\t" // iterate again if i != 0.
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	".DCONSIDKLEFT:                              \n\t"
	"                                            \n\t"
	"movq      %[k_left], %%rsi                  \n\t" // i = k_left;
	"testq  %%rsi, %%rsi                         \n\t" // check i via logical AND.
	"je     .DPOSTACCUM                          \n\t" // if i == 0, we're done; jump to end.
	"                                            \n\t" // else, we prepare to enter k_left loop.
	"                                            \n\t"
	"                                            \n\t"
	".DLOOPKLEFT:                                \n\t" // EDGE LOOP
	"                                            \n\t"
	"vmovapd   1 * 32(%%rax),  %%ymm1            \n\t" // preload a47 
	"addq         $8 * 1 * 8,  %%rax             \n\t" // a += 8 (1 x mr)
	"vmulpd           %%ymm0,  %%ymm2, %%ymm6    \n\t"
	"vperm2f128 $0x3, %%ymm2,  %%ymm2, %%ymm4    \n\t"
	"vmulpd           %%ymm0,  %%ymm3, %%ymm7    \n\t"
	"vperm2f128 $0x3, %%ymm3,  %%ymm3, %%ymm5    \n\t"
	"vaddpd           %%ymm15, %%ymm6, %%ymm15   \n\t"
	"vaddpd           %%ymm13, %%ymm7, %%ymm13   \n\t"
	"                                            \n\t"
	"prefetcht0  14 * 32(%%rax)                  \n\t" // prefetch a03 for iter 7 later ( ? )
	"vmulpd           %%ymm1,  %%ymm2, %%ymm6    \n\t"
	"vmovapd   1 * 32(%%rbx),  %%ymm2            \n\t"
	"addq         $4 * 1 * 8,  %%rbx             \n\t" // b += 4 (1 x nr)
	"vmulpd           %%ymm1,  %%ymm3, %%ymm7    \n\t"
	"vpermilpd  $0x5, %%ymm2,  %%ymm3            \n\t"
	"vaddpd           %%ymm14, %%ymm6, %%ymm14   \n\t"
	"vaddpd           %%ymm12, %%ymm7, %%ymm12   \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm0,  %%ymm4, %%ymm6    \n\t"
	"vmulpd           %%ymm0,  %%ymm5, %%ymm7    \n\t"
	"vmovapd   0 * 32(%%rax),  %%ymm0            \n\t"
	"vaddpd           %%ymm11, %%ymm6, %%ymm11   \n\t"
	"vaddpd           %%ymm9,  %%ymm7, %%ymm9    \n\t"
	"                                            \n\t"
	"vmulpd           %%ymm1,  %%ymm4, %%ymm6    \n\t"
	"vmulpd           %%ymm1,  %%ymm5, %%ymm7    \n\t"
	"vaddpd           %%ymm10, %%ymm6, %%ymm10   \n\t"
	"vaddpd           %%ymm8,  %%ymm7, %%ymm8    \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"decq   %%rsi                                \n\t" // i -= 1;
	"jne    .DLOOPKLEFT                          \n\t" // iterate again if i != 0.
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t"
	".DPOSTACCUM:                                \n\t"
	"                                            \n\t"
	"                                            \n\t"
	"                                            \n\t" // ymm15:  ymm13:  ymm11:  ymm9:
	"                                            \n\t" // ( ab00  ( ab01  ( ab02  ( ab03
	"                                            \n\t" //   ab11    ab10    ab13    ab12  
	"                                            \n\t" //   ab22    ab23    ab20    ab21
	"                                            \n\t" //   ab33 )  ab32 )  ab31 )  ab30 )
	"                                            \n\t"
	"                                            \n\t" // ymm14:  ymm12:  ymm10:  ymm8:
	"                                            \n\t" // ( ab40  ( ab41  ( ab42  ( ab43
	"                                            \n\t" //   ab51    ab50    ab53    ab52  
	"                                            \n\t" //   ab62    ab63    ab60    ab61
	"                                            \n\t" //   ab73 )  ab72 )  ab71 )  ab70 )
	"                                            \n\t"
	"vmovapd          %%ymm15, %%ymm7            \n\t"
	"vshufpd    $0xa, %%ymm15, %%ymm13, %%ymm15  \n\t"
	"vshufpd    $0xa, %%ymm13, %%ymm7,  %%ymm13  \n\t"
	"                                            \n\t"
	"vmovapd          %%ymm11, %%ymm7            \n\t"
	"vshufpd    $0xa, %%ymm11, %%ymm9,  %%ymm11  \n\t"
	"vshufpd    $0xa, %%ymm9,  %%ymm7,  %%ymm9   \n\t"
	"                                            \n\t"
	"vmovapd          %%ymm14, %%ymm7            \n\t"
	"vshufpd    $0xa, %%ymm14, %%ymm12, %%ymm14  \n\t"
	"vshufpd    $0xa, %%ymm12, %%ymm7,  %%ymm12  \n\t"
	"                                            \n\t"
	"vmovapd          %%ymm10, %%ymm7            \n\t"
	"vshufpd    $0xa, %%ymm10, %%ymm8,  %%ymm10  \n\t"
	"vshufpd    $0xa, %%ymm8,  %%ymm7,  %%ymm8   \n\t"
	"                                            \n\t"
	"                                            \n\t" // ymm15:  ymm13:  ymm11:  ymm9:
	"                                            \n\t" // ( ab01  ( ab00  ( ab03  ( ab02
	"                                            \n\t" //   ab11    ab10    ab13    ab12  
	"                                            \n\t" //   ab23    ab22    ab21    ab20
	"                                            \n\t" //   ab33 )  ab32 )  ab31 )  ab30 )
	"                                            \n\t"
	"                                            \n\t" // ymm14:  ymm12:  ymm10:  ymm8:
	"                                            \n\t" // ( ab41  ( ab40  ( ab43  ( ab42
	"                                            \n\t" //   ab51    ab50    ab53    ab52  
	"                                            \n\t" //   ab63    ab62    ab61    ab60
	"                                            \n\t" //   ab73 )  ab72 )  ab71 )  ab70 )
	"                                            \n\t"
	"vmovapd           %%ymm15, %%ymm7           \n\t"
	"vperm2f128 $0x30, %%ymm15, %%ymm11, %%ymm15 \n\t"
	"vperm2f128 $0x12, %%ymm7,  %%ymm11, %%ymm11 \n\t"
	"                                            \n\t"
	"vmovapd           %%ymm13, %%ymm7           \n\t"
	"vperm2f128 $0x30, %%ymm13, %%ymm9,  %%ymm13 \n\t"
	"vperm2f128 $0x12, %%ymm7,  %%ymm9,  %%ymm9  \n\t"
	"                                            \n\t"
	"vmovapd           %%ymm14, %%ymm7           \n\t"
	"vperm2f128 $0x30, %%ymm14, %%ymm10, %%ymm14 \n\t"
	"vperm2f128 $0x12, %%ymm7,  %%ymm10, %%ymm10 \n\t"
	"                                            \n\t"
	"vmovapd           %%ymm12, %%ymm7           \n\t"
	"vperm2f128 $0x30, %%ymm12, %%ymm8,  %%ymm12 \n\t"
	"vperm2f128 $0x12, %%ymm7,  %%ymm8,  %%ymm8  \n\t"
	"                                            \n\t"
	"                                            \n\t" // ymm9:   ymm11:  ymm13:  ymm15:
	"                                            \n\t" // ( ab00  ( ab01  ( ab02  ( ab03
	"                                            \n\t" //   ab10    ab11    ab12    ab13  
	"                                            \n\t" //   ab20    ab21    ab22    ab23
	"                                            \n\t" //   ab30 )  ab31 )  ab32 )  ab33 )
	"                                            \n\t"
	"                                            \n\t" // ymm8:   ymm10:  ymm12:  ymm14:
	"                                            \n\t" // ( ab40  ( ab41  ( ab42  ( ab43
	"                                            \n\t" //   ab50    ab51    ab52    ab53  
	"                                            \n\t" //   ab60    ab61    ab62    ab63
	"                                            \n\t" //   ab70 )  ab71 )  ab72 )  ab73 )
	"                                            \n\t"
    "                                            \n\t"
    "movq         %[rs_c], %%rsi                 \n\t" // load rs_c
    "                                            \n\t"
    "leaq        (,%%rsi,8), %%rsi               \n\t" // rsi = rs_c * sizeof(double)
    "                                            \n\t"
    "                                            \n\t"
    "                                            \n\t" // determine if
    "                                            \n\t" //    c    % 32 == 0, AND
    "                                            \n\t" //  8*cs_c % 32 == 0, AND
    "                                            \n\t" //    rs_c      == 1
    "                                            \n\t" // ie: aligned, ldim aligned, and
    "                                            \n\t" // column-stored
    "                                            \n\t"
    "cmpq       $8, %%rsi                        \n\t" // set ZF if (8*rs_c) == 8.
    "sete           %%bl                         \n\t" // bl = ( ZF == 1 ? 1 : 0 );
    "testq     $31, %%rcx                        \n\t" // set ZF if c_list[ 0 ] & 32 is zero.
    "setz           %%bh                         \n\t" // bh = ( ZF == 0 ? 1 : 0 );
    "testq     $31, %%rdi                        \n\t" // set ZF if (8*cs_c) & 32 is zero.
    "setz           %%al                         \n\t" // al = ( ZF == 0 ? 1 : 0 );
    "                                            \n\t" // and(bl,bh) followed by
    "                                            \n\t" // and(bh,al) will reveal result
    "                                            \n\t"
    //"jmp     .DCOLSTORED                         \n\t" // jump to column storage case
    //"jmp     .DGENSTORED                         \n\t" // jump to column storage case
    "                                            \n\t"
    "                                            \n\t" // now avoid loading C if beta == 0
    "                                            \n\t"
//    "vxorpd    %%ymm0,  %%ymm0,  %%ymm0          \n\t" // set ymm0 to zero.
//    "vucomisd  %%xmm0,  %%xmm2                   \n\t" // set ZF if beta == 0.
//    "je      .DBETAZERO                          \n\t" // if ZF = 1, jump to beta == 0 case
    "                                            \n\t"
    "                                            \n\t" // check if aligned/column-stored
    "andb     %%bl, %%bh                         \n\t" // set ZF if bl & bh == 1.
    "andb     %%bh, %%al                         \n\t" // set ZF if bh & al == 1.
    "jne     .DCOLSTORED                         \n\t" // jump to column storage case
    "                                            \n\t"
    ".DGENSTORED:                                \n\t"
    "                                            \n\t"
    "leaq        (,%%rsi,2), %%r12               \n\t" // r12 = 2*rs_c;
    "leaq   (%%r12,%%rsi,1), %%r13               \n\t" // r13 = 3*rs_c;
    "                                            \n\t"
    "movq         %[coeff0], %%rbx               \n\t" // load address of coeff0
    "vbroadcastsd    (%%rbx), %%ymm0             \n\t" // load coeff0 and duplicate
    "leaq   (%%rcx,%%rsi,4), %%rax               \n\t" // load address of c0 + 4*rs_c;'
    "                                            \n\t"
    "vextractf128 $1, %%ymm9,  %%xmm2            \n\t"
    "vmovlpd    (%%rcx),       %%xmm1,  %%xmm1   \n\t" // load c0_00 and c0_10,
    "vmovhpd    (%%rcx,%%rsi), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm9,  %%xmm3   \n\t" // scale by coeff0,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%rcx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%rcx,%%rsi)     \n\t"
    "vmovlpd    (%%rcx,%%r12), %%xmm1,  %%xmm1   \n\t" // load c0_20 and c0_30,
    "vmovhpd    (%%rcx,%%r13), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm2,  %%xmm3   \n\t" // scale by coeff0,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%rcx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%rcx,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rcx                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm11,  %%xmm5            \n\t"
    "vmovlpd    (%%rcx),       %%xmm4,  %%xmm4   \n\t" // load c0_01 and c0_11,
    "vmovhpd    (%%rcx,%%rsi), %%xmm4,  %%xmm4   \n\t"
    "vmulpd           %%xmm0,  %%xmm11,  %%xmm6   \n\t" // scale by coeff0,
    "vaddpd           %%xmm6,  %%xmm4,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%rcx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%rcx,%%rsi)     \n\t"
    "vmovlpd    (%%rcx,%%r12), %%xmm4,  %%xmm4   \n\t" // load c0_21 and c0_31,
    "vmovhpd    (%%rcx,%%r13), %%xmm4,  %%xmm4   \n\t"
    "vmulpd           %%xmm0,  %%xmm5,  %%xmm6   \n\t" // scale by coeff0,
    "vaddpd           %%xmm6,  %%xmm4,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%rcx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%rcx,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rcx                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm13,  %%xmm1            \n\t"
    "vmovlpd    (%%rcx),       %%xmm7,  %%xmm7   \n\t" // load c0_02 and c0_12,
    "vmovhpd    (%%rcx,%%rsi), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm0,  %%xmm13,  %%xmm2   \n\t" // scale by coeff0,
    "vaddpd           %%xmm2,  %%xmm7,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%rcx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%rcx,%%rsi)     \n\t"
    "vmovlpd    (%%rcx,%%r12), %%xmm7,  %%xmm7   \n\t" // load c0_22 and c0_32,
    "vmovhpd    (%%rcx,%%r13), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm0,  %%xmm1,  %%xmm2   \n\t" // scale by coeff0,
    "vaddpd           %%xmm2,  %%xmm7,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%rcx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%rcx,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rcx                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm15,  %%xmm4            \n\t"
    "vmovlpd    (%%rcx),       %%xmm3,  %%xmm3   \n\t" // load c0_03 and c0_13,
    "vmovhpd    (%%rcx,%%rsi), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm0,  %%xmm15,  %%xmm5   \n\t" // scale by coeff0,
    "vaddpd           %%xmm5,  %%xmm3,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%rcx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%rcx,%%rsi)     \n\t"
    "vmovlpd    (%%rcx,%%r12), %%xmm3,  %%xmm3   \n\t" // load c0_23 and c0_33,
    "vmovhpd    (%%rcx,%%r13), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm0,  %%xmm4,  %%xmm5   \n\t" // scale by coeff0,
    "vaddpd           %%xmm5,  %%xmm3,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%rcx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%rcx,%%r13)     \n\t"
    "                                            \n\t"
    "vextractf128 $1, %%ymm8,  %%xmm7            \n\t"
    "vmovlpd    (%%rax),       %%xmm6,  %%xmm6   \n\t" // load c0_40 and c0_50,
    "vmovhpd    (%%rax,%%rsi), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm0,  %%xmm8,  %%xmm1   \n\t" // scale by coeff0,
    "vaddpd           %%xmm1,  %%xmm6,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm6,  %%xmm6   \n\t" // load c0_60 and c0_70,
    "vmovhpd    (%%rax,%%r13), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm0,  %%xmm7,  %%xmm1   \n\t" // scale by coeff0,
    "vaddpd           %%xmm1,  %%xmm6,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rax                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm10,  %%xmm3            \n\t"
    "vmovlpd    (%%rax),       %%xmm2,  %%xmm2   \n\t" // load c0_41 and c0_51,
    "vmovhpd    (%%rax,%%rsi), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm0,  %%xmm10,  %%xmm4   \n\t" // scale by coeff0,
    "vaddpd           %%xmm4,  %%xmm2,  %%xmm4   \n\t" // add the gemm result,
    "vmovlpd          %%xmm4,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm4,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm2,  %%xmm2   \n\t" // load c0_61 and c0_71,
    "vmovhpd    (%%rax,%%r13), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm0,  %%xmm3,  %%xmm4   \n\t" // scale by coeff0,
    "vaddpd           %%xmm4,  %%xmm2,  %%xmm4   \n\t" // add the gemm result,
    "vmovlpd          %%xmm4,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm4,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rax                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm12,  %%xmm6            \n\t"
    "vmovlpd    (%%rax),       %%xmm5,  %%xmm5   \n\t" // load c0_42 and c0_52,
    "vmovhpd    (%%rax,%%rsi), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm0,  %%xmm12,  %%xmm7   \n\t" // scale by coeff0,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm5,  %%xmm5   \n\t" // load c0_62 and c0_72,
    "vmovhpd    (%%rax,%%r13), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm0,  %%xmm6,  %%xmm7   \n\t" // scale by coeff0,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rax                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm14,  %%xmm2            \n\t"
    "vmovlpd    (%%rax),       %%xmm1,  %%xmm1   \n\t" // load c0_43 and c0_53,
    "vmovhpd    (%%rax,%%rsi), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm14,  %%xmm3   \n\t" // scale by coeff0,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm1,  %%xmm1   \n\t" // load c0_63 and c0_73,
    "vmovhpd    (%%rax,%%r13), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm2,  %%xmm3   \n\t" // scale by coeff0,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "movq         %[coeff1], %%r9               \n\t" // load address of coeff1
    "vbroadcastsd    (%%r9), %%ymm4             \n\t" // load coeff1 and duplicate
    "leaq   (%%rdx,%%rsi,4), %%r8               \n\t" // load address of c1 + 4*rs_c;'
    "                                            \n\t"
    "vextractf128 $1, %%ymm9,  %%xmm6            \n\t"
    "vmovlpd    (%%rdx),       %%xmm5,  %%xmm5   \n\t" // load c1_00 and c1_10,
    "vmovhpd    (%%rdx,%%rsi), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm9,  %%xmm7   \n\t" // scale by coeff1,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%rdx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%rdx,%%rsi)     \n\t"
    "vmovlpd    (%%rdx,%%r12), %%xmm5,  %%xmm5   \n\t" // load c1_20 and c1_30,
    "vmovhpd    (%%rdx,%%r13), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm6,  %%xmm7   \n\t" // scale by coeff1,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%rdx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%rdx,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rdx                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm11,  %%xmm1            \n\t"
    "vmovlpd    (%%rdx),       %%xmm0,  %%xmm0   \n\t" // load c1_01 and c1_11,
    "vmovhpd    (%%rdx,%%rsi), %%xmm0,  %%xmm0   \n\t"
    "vmulpd           %%xmm4,  %%xmm11,  %%xmm2   \n\t" // scale by coeff1,
    "vaddpd           %%xmm2,  %%xmm0,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%rdx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%rdx,%%rsi)     \n\t"
    "vmovlpd    (%%rdx,%%r12), %%xmm0,  %%xmm0   \n\t" // load c1_21 and c1_31,
    "vmovhpd    (%%rdx,%%r13), %%xmm0,  %%xmm0   \n\t"
    "vmulpd           %%xmm4,  %%xmm1,  %%xmm2   \n\t" // scale by coeff1,
    "vaddpd           %%xmm2,  %%xmm0,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%rdx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%rdx,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rdx                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm13,  %%xmm5            \n\t"
    "vmovlpd    (%%rdx),       %%xmm3,  %%xmm3   \n\t" // load c1_02 and c1_12,
    "vmovhpd    (%%rdx,%%rsi), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm4,  %%xmm13,  %%xmm6   \n\t" // scale by coeff1,
    "vaddpd           %%xmm6,  %%xmm3,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%rdx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%rdx,%%rsi)     \n\t"
    "vmovlpd    (%%rdx,%%r12), %%xmm3,  %%xmm3   \n\t" // load c1_22 and c1_32,
    "vmovhpd    (%%rdx,%%r13), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm4,  %%xmm5,  %%xmm6   \n\t" // scale by coeff1,
    "vaddpd           %%xmm6,  %%xmm3,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%rdx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%rdx,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rdx                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm15,  %%xmm0            \n\t"
    "vmovlpd    (%%rdx),       %%xmm7,  %%xmm7   \n\t" // load c1_03 and c1_13,
    "vmovhpd    (%%rdx,%%rsi), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm4,  %%xmm15,  %%xmm1   \n\t" // scale by coeff1,
    "vaddpd           %%xmm1,  %%xmm7,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%rdx)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%rdx,%%rsi)     \n\t"
    "vmovlpd    (%%rdx,%%r12), %%xmm7,  %%xmm7   \n\t" // load c1_23 and c1_33,
    "vmovhpd    (%%rdx,%%r13), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm4,  %%xmm0,  %%xmm1   \n\t" // scale by coeff1,
    "vaddpd           %%xmm1,  %%xmm7,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%rdx,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%rdx,%%r13)     \n\t"
    "                                            \n\t"
    "vextractf128 $1, %%ymm8,  %%xmm3            \n\t"
    "vmovlpd    (%%r8),       %%xmm2,  %%xmm2   \n\t" // load c1_40 and c1_50,
    "vmovhpd    (%%r8,%%rsi), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm4,  %%xmm8,  %%xmm5   \n\t" // scale by coeff1,
    "vaddpd           %%xmm5,  %%xmm2,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%r8)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%r8,%%rsi)     \n\t"
    "vmovlpd    (%%r8,%%r12), %%xmm2,  %%xmm2   \n\t" // load c1_60 and c1_70,
    "vmovhpd    (%%r8,%%r13), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm4,  %%xmm3,  %%xmm5   \n\t" // scale by coeff1,
    "vaddpd           %%xmm5,  %%xmm2,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%r8,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%r8,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r8                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm10,  %%xmm7            \n\t"
    "vmovlpd    (%%r8),       %%xmm6,  %%xmm6   \n\t" // load c1_41 and c1_51,
    "vmovhpd    (%%r8,%%rsi), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm4,  %%xmm10,  %%xmm0   \n\t" // scale by coeff1,
    "vaddpd           %%xmm0,  %%xmm6,  %%xmm0   \n\t" // add the gemm result,
    "vmovlpd          %%xmm0,  (%%r8)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm0,  (%%r8,%%rsi)     \n\t"
    "vmovlpd    (%%r8,%%r12), %%xmm6,  %%xmm6   \n\t" // load c1_61 and c1_71,
    "vmovhpd    (%%r8,%%r13), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm4,  %%xmm7,  %%xmm0   \n\t" // scale by coeff1,
    "vaddpd           %%xmm0,  %%xmm6,  %%xmm0   \n\t" // add the gemm result,
    "vmovlpd          %%xmm0,  (%%r8,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm0,  (%%r8,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r8                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm12,  %%xmm2            \n\t"
    "vmovlpd    (%%r8),       %%xmm1,  %%xmm1   \n\t" // load c1_42 and c1_52,
    "vmovhpd    (%%r8,%%rsi), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm4,  %%xmm12,  %%xmm3   \n\t" // scale by coeff1,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%r8)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%r8,%%rsi)     \n\t"
    "vmovlpd    (%%r8,%%r12), %%xmm1,  %%xmm1   \n\t" // load c1_62 and c1_72,
    "vmovhpd    (%%r8,%%r13), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm4,  %%xmm2,  %%xmm3   \n\t" // scale by coeff1,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%r8,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%r8,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r8                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm14,  %%xmm6            \n\t"
    "vmovlpd    (%%r8),       %%xmm5,  %%xmm5   \n\t" // load c1_43 and c1_53,
    "vmovhpd    (%%r8,%%rsi), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm14,  %%xmm7   \n\t" // scale by coeff1,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%r8)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%r8,%%rsi)     \n\t"
    "vmovlpd    (%%r8,%%r12), %%xmm5,  %%xmm5   \n\t" // load c1_63 and c1_73,
    "vmovhpd    (%%r8,%%r13), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm6,  %%xmm7   \n\t" // scale by coeff1,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%r8,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%r8,%%r13)     \n\t"
    "                                            \n\t"
    "movq         %[coeff2], %%r15               \n\t" // load address of coeff2
    "vbroadcastsd    (%%r15), %%ymm0             \n\t" // load coeff2 and duplicate
    "leaq   (%%r10,%%rsi,4), %%r11               \n\t" // load address of c2 + 4*rs_c;'
    "                                            \n\t"
    "vextractf128 $1, %%ymm9,  %%xmm2            \n\t"
    "vmovlpd    (%%r10),       %%xmm1,  %%xmm1   \n\t" // load c2_00 and c2_10,
    "vmovhpd    (%%r10,%%rsi), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm9,  %%xmm3   \n\t" // scale by coeff2,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%r10)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%r10,%%rsi)     \n\t"
    "vmovlpd    (%%r10,%%r12), %%xmm1,  %%xmm1   \n\t" // load c2_20 and c2_30,
    "vmovhpd    (%%r10,%%r13), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm2,  %%xmm3   \n\t" // scale by coeff2,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%r10,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%r10,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r10                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm11,  %%xmm5            \n\t"
    "vmovlpd    (%%r10),       %%xmm4,  %%xmm4   \n\t" // load c2_01 and c2_11,
    "vmovhpd    (%%r10,%%rsi), %%xmm4,  %%xmm4   \n\t"
    "vmulpd           %%xmm0,  %%xmm11,  %%xmm6   \n\t" // scale by coeff2,
    "vaddpd           %%xmm6,  %%xmm4,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%r10)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%r10,%%rsi)     \n\t"
    "vmovlpd    (%%r10,%%r12), %%xmm4,  %%xmm4   \n\t" // load c2_21 and c2_31,
    "vmovhpd    (%%r10,%%r13), %%xmm4,  %%xmm4   \n\t"
    "vmulpd           %%xmm0,  %%xmm5,  %%xmm6   \n\t" // scale by coeff2,
    "vaddpd           %%xmm6,  %%xmm4,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%r10,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%r10,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r10                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm13,  %%xmm1            \n\t"
    "vmovlpd    (%%r10),       %%xmm7,  %%xmm7   \n\t" // load c2_02 and c2_12,
    "vmovhpd    (%%r10,%%rsi), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm0,  %%xmm13,  %%xmm2   \n\t" // scale by coeff2,
    "vaddpd           %%xmm2,  %%xmm7,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%r10)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%r10,%%rsi)     \n\t"
    "vmovlpd    (%%r10,%%r12), %%xmm7,  %%xmm7   \n\t" // load c2_22 and c2_32,
    "vmovhpd    (%%r10,%%r13), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm0,  %%xmm1,  %%xmm2   \n\t" // scale by coeff2,
    "vaddpd           %%xmm2,  %%xmm7,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%r10,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%r10,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r10                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm15,  %%xmm4            \n\t"
    "vmovlpd    (%%r10),       %%xmm3,  %%xmm3   \n\t" // load c2_03 and c2_13,
    "vmovhpd    (%%r10,%%rsi), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm0,  %%xmm15,  %%xmm5   \n\t" // scale by coeff2,
    "vaddpd           %%xmm5,  %%xmm3,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%r10)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%r10,%%rsi)     \n\t"
    "vmovlpd    (%%r10,%%r12), %%xmm3,  %%xmm3   \n\t" // load c2_23 and c2_33,
    "vmovhpd    (%%r10,%%r13), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm0,  %%xmm4,  %%xmm5   \n\t" // scale by coeff2,
    "vaddpd           %%xmm5,  %%xmm3,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%r10,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%r10,%%r13)     \n\t"
    "                                            \n\t"
    "vextractf128 $1, %%ymm8,  %%xmm7            \n\t"
    "vmovlpd    (%%r11),       %%xmm6,  %%xmm6   \n\t" // load c2_40 and c2_50,
    "vmovhpd    (%%r11,%%rsi), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm0,  %%xmm8,  %%xmm1   \n\t" // scale by coeff2,
    "vaddpd           %%xmm1,  %%xmm6,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%r11)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%r11,%%rsi)     \n\t"
    "vmovlpd    (%%r11,%%r12), %%xmm6,  %%xmm6   \n\t" // load c2_60 and c2_70,
    "vmovhpd    (%%r11,%%r13), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm0,  %%xmm7,  %%xmm1   \n\t" // scale by coeff2,
    "vaddpd           %%xmm1,  %%xmm6,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%r11,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%r11,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r11                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm10,  %%xmm3            \n\t"
    "vmovlpd    (%%r11),       %%xmm2,  %%xmm2   \n\t" // load c2_41 and c2_51,
    "vmovhpd    (%%r11,%%rsi), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm0,  %%xmm10,  %%xmm4   \n\t" // scale by coeff2,
    "vaddpd           %%xmm4,  %%xmm2,  %%xmm4   \n\t" // add the gemm result,
    "vmovlpd          %%xmm4,  (%%r11)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm4,  (%%r11,%%rsi)     \n\t"
    "vmovlpd    (%%r11,%%r12), %%xmm2,  %%xmm2   \n\t" // load c2_61 and c2_71,
    "vmovhpd    (%%r11,%%r13), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm0,  %%xmm3,  %%xmm4   \n\t" // scale by coeff2,
    "vaddpd           %%xmm4,  %%xmm2,  %%xmm4   \n\t" // add the gemm result,
    "vmovlpd          %%xmm4,  (%%r11,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm4,  (%%r11,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r11                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm12,  %%xmm6            \n\t"
    "vmovlpd    (%%r11),       %%xmm5,  %%xmm5   \n\t" // load c2_42 and c2_52,
    "vmovhpd    (%%r11,%%rsi), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm0,  %%xmm12,  %%xmm7   \n\t" // scale by coeff2,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%r11)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%r11,%%rsi)     \n\t"
    "vmovlpd    (%%r11,%%r12), %%xmm5,  %%xmm5   \n\t" // load c2_62 and c2_72,
    "vmovhpd    (%%r11,%%r13), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm0,  %%xmm6,  %%xmm7   \n\t" // scale by coeff2,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%r11,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%r11,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r11                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm14,  %%xmm2            \n\t"
    "vmovlpd    (%%r11),       %%xmm1,  %%xmm1   \n\t" // load c2_43 and c2_53,
    "vmovhpd    (%%r11,%%rsi), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm14,  %%xmm3   \n\t" // scale by coeff2,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%r11)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%r11,%%rsi)     \n\t"
    "vmovlpd    (%%r11,%%r12), %%xmm1,  %%xmm1   \n\t" // load c2_63 and c2_73,
    "vmovhpd    (%%r11,%%r13), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm0,  %%xmm2,  %%xmm3   \n\t" // scale by coeff2,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%r11,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%r11,%%r13)     \n\t"
    "                                            \n\t"
    "movq         %[coeff3], %%rbx               \n\t" // load address of coeff3
    "vbroadcastsd    (%%rbx), %%ymm4             \n\t" // load coeff3 and duplicate
    "leaq   (%%r14,%%rsi,4), %%rax               \n\t" // load address of c3 + 4*rs_c;'
    "                                            \n\t"
    "vextractf128 $1, %%ymm9,  %%xmm6            \n\t"
    "vmovlpd    (%%r14),       %%xmm5,  %%xmm5   \n\t" // load c3_00 and c3_10,
    "vmovhpd    (%%r14,%%rsi), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm9,  %%xmm7   \n\t" // scale by coeff3,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%r14)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%r14,%%rsi)     \n\t"
    "vmovlpd    (%%r14,%%r12), %%xmm5,  %%xmm5   \n\t" // load c3_20 and c3_30,
    "vmovhpd    (%%r14,%%r13), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm6,  %%xmm7   \n\t" // scale by coeff3,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%r14,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%r14,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r14                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm11,  %%xmm1            \n\t"
    "vmovlpd    (%%r14),       %%xmm0,  %%xmm0   \n\t" // load c3_01 and c3_11,
    "vmovhpd    (%%r14,%%rsi), %%xmm0,  %%xmm0   \n\t"
    "vmulpd           %%xmm4,  %%xmm11,  %%xmm2   \n\t" // scale by coeff3,
    "vaddpd           %%xmm2,  %%xmm0,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%r14)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%r14,%%rsi)     \n\t"
    "vmovlpd    (%%r14,%%r12), %%xmm0,  %%xmm0   \n\t" // load c3_21 and c3_31,
    "vmovhpd    (%%r14,%%r13), %%xmm0,  %%xmm0   \n\t"
    "vmulpd           %%xmm4,  %%xmm1,  %%xmm2   \n\t" // scale by coeff3,
    "vaddpd           %%xmm2,  %%xmm0,  %%xmm2   \n\t" // add the gemm result,
    "vmovlpd          %%xmm2,  (%%r14,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm2,  (%%r14,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r14                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm13,  %%xmm5            \n\t"
    "vmovlpd    (%%r14),       %%xmm3,  %%xmm3   \n\t" // load c3_02 and c3_12,
    "vmovhpd    (%%r14,%%rsi), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm4,  %%xmm13,  %%xmm6   \n\t" // scale by coeff3,
    "vaddpd           %%xmm6,  %%xmm3,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%r14)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%r14,%%rsi)     \n\t"
    "vmovlpd    (%%r14,%%r12), %%xmm3,  %%xmm3   \n\t" // load c3_22 and c3_32,
    "vmovhpd    (%%r14,%%r13), %%xmm3,  %%xmm3   \n\t"
    "vmulpd           %%xmm4,  %%xmm5,  %%xmm6   \n\t" // scale by coeff3,
    "vaddpd           %%xmm6,  %%xmm3,  %%xmm6   \n\t" // add the gemm result,
    "vmovlpd          %%xmm6,  (%%r14,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm6,  (%%r14,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%r14                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm15,  %%xmm0            \n\t"
    "vmovlpd    (%%r14),       %%xmm7,  %%xmm7   \n\t" // load c3_03 and c3_13,
    "vmovhpd    (%%r14,%%rsi), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm4,  %%xmm15,  %%xmm1   \n\t" // scale by coeff3,
    "vaddpd           %%xmm1,  %%xmm7,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%r14)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%r14,%%rsi)     \n\t"
    "vmovlpd    (%%r14,%%r12), %%xmm7,  %%xmm7   \n\t" // load c3_23 and c3_33,
    "vmovhpd    (%%r14,%%r13), %%xmm7,  %%xmm7   \n\t"
    "vmulpd           %%xmm4,  %%xmm0,  %%xmm1   \n\t" // scale by coeff3,
    "vaddpd           %%xmm1,  %%xmm7,  %%xmm1   \n\t" // add the gemm result,
    "vmovlpd          %%xmm1,  (%%r14,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm1,  (%%r14,%%r13)     \n\t"
    "                                            \n\t"
    "vextractf128 $1, %%ymm8,  %%xmm3            \n\t"
    "vmovlpd    (%%rax),       %%xmm2,  %%xmm2   \n\t" // load c3_40 and c3_50,
    "vmovhpd    (%%rax,%%rsi), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm4,  %%xmm8,  %%xmm5   \n\t" // scale by coeff3,
    "vaddpd           %%xmm5,  %%xmm2,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm2,  %%xmm2   \n\t" // load c3_60 and c3_70,
    "vmovhpd    (%%rax,%%r13), %%xmm2,  %%xmm2   \n\t"
    "vmulpd           %%xmm4,  %%xmm3,  %%xmm5   \n\t" // scale by coeff3,
    "vaddpd           %%xmm5,  %%xmm2,  %%xmm5   \n\t" // add the gemm result,
    "vmovlpd          %%xmm5,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm5,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rax                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm10,  %%xmm7            \n\t"
    "vmovlpd    (%%rax),       %%xmm6,  %%xmm6   \n\t" // load c3_41 and c3_51,
    "vmovhpd    (%%rax,%%rsi), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm4,  %%xmm10,  %%xmm0   \n\t" // scale by coeff3,
    "vaddpd           %%xmm0,  %%xmm6,  %%xmm0   \n\t" // add the gemm result,
    "vmovlpd          %%xmm0,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm0,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm6,  %%xmm6   \n\t" // load c3_61 and c3_71,
    "vmovhpd    (%%rax,%%r13), %%xmm6,  %%xmm6   \n\t"
    "vmulpd           %%xmm4,  %%xmm7,  %%xmm0   \n\t" // scale by coeff3,
    "vaddpd           %%xmm0,  %%xmm6,  %%xmm0   \n\t" // add the gemm result,
    "vmovlpd          %%xmm0,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm0,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rax                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm12,  %%xmm2            \n\t"
    "vmovlpd    (%%rax),       %%xmm1,  %%xmm1   \n\t" // load c3_42 and c3_52,
    "vmovhpd    (%%rax,%%rsi), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm4,  %%xmm12,  %%xmm3   \n\t" // scale by coeff3,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm1,  %%xmm1   \n\t" // load c3_62 and c3_72,
    "vmovhpd    (%%rax,%%r13), %%xmm1,  %%xmm1   \n\t"
    "vmulpd           %%xmm4,  %%xmm2,  %%xmm3   \n\t" // scale by coeff3,
    "vaddpd           %%xmm3,  %%xmm1,  %%xmm3   \n\t" // add the gemm result,
    "vmovlpd          %%xmm3,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm3,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "addq      %%rdi, %%rax                      \n\t" // c += cs_c;
    "vextractf128 $1, %%ymm14,  %%xmm6            \n\t"
    "vmovlpd    (%%rax),       %%xmm5,  %%xmm5   \n\t" // load c3_43 and c3_53,
    "vmovhpd    (%%rax,%%rsi), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm14,  %%xmm7   \n\t" // scale by coeff3,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%rax)           \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%rax,%%rsi)     \n\t"
    "vmovlpd    (%%rax,%%r12), %%xmm5,  %%xmm5   \n\t" // load c3_63 and c3_73,
    "vmovhpd    (%%rax,%%r13), %%xmm5,  %%xmm5   \n\t"
    "vmulpd           %%xmm4,  %%xmm6,  %%xmm7   \n\t" // scale by coeff3,
    "vaddpd           %%xmm7,  %%xmm5,  %%xmm7   \n\t" // add the gemm result,
    "vmovlpd          %%xmm7,  (%%rax,%%r12)     \n\t" // and store back to memory.
    "vmovhpd          %%xmm7,  (%%rax,%%r13)     \n\t"
    "                                            \n\t"
    "                                            \n\t"
    "jmp    .DDONE                               \n\t" // jump to end.
    "                                            \n\t"
    ".DCOLSTORED:                                \n\t"
    "                                            \n\t"
    "movq         %[coeff0], %%rax                      \n\t" // load address of alpha_list[ 0 ]
	"movq         %[coeff1], %%rbx                      \n\t" // load address of alpha_list[ 1 ]
	"movq         %[coeff2], %%r12                      \n\t" // load address of alpha_list[ 2 ]
	"movq         %[coeff3], %%r11                      \n\t" // load address of alpha_list[ 3 ]
	"vbroadcastsd    (%%rax), %%ymm7             \n\t" // load alpha_list[ 0 ] and duplicate
	"vbroadcastsd    (%%rbx), %%ymm6             \n\t" // load alpha_list[ 1 ] and duplicate
	"vbroadcastsd    (%%r12), %%ymm5             \n\t" // load alpha_list[ 2 ] and duplicate
	"vbroadcastsd    (%%r11), %%ymm4             \n\t" // load alpha_list[ 3 ] and duplicate
    "                                            \n\t"
    "                                            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rcx),  %%ymm0           \n\t" // ymm0 = c_list[0]( 0:3, 0 )
	"vmulpd            %%ymm7,  %%ymm9,  %%ymm1  \n\t" // scale by alpha1, ymm1 = ymm7( alpha1 ) * ymm9( ab0_3:0 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 0:3, 0 )
	"vmovapd    1 * 32(%%rcx),  %%ymm3           \n\t" // ymm3 = c_list[0]( 4:7, 0 )
	"vmulpd            %%ymm7,  %%ymm8,  %%ymm2  \n\t" // scale by alpha1, ymm2 = ymm7( alpha1 ) * ymm8( ab4_7:0 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 4:7, 0 )
	"addq              %%rdi,   %%rcx            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rcx),  %%ymm0           \n\t" // ymm0 = c_list[0]( 0:3, 0 )
	"vmulpd            %%ymm7,  %%ymm11, %%ymm1  \n\t" // scale by alpha1, ymm1 = ymm7( alpha1 ) * ymm11( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 0:3, 0 )
	"vmovapd    1 * 32(%%rcx),  %%ymm3           \n\t" // ymm3 = c_list[0]( 4:7, 0 )
	"vmulpd            %%ymm7,  %%ymm10, %%ymm2  \n\t" // scale by alpha1, ymm2 = ymm7( alpha1 ) * ymm10( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 4:7, 0 )
	"addq              %%rdi,   %%rcx            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rcx),  %%ymm0           \n\t" // ymm0 = c_list[0]( 0:3, 0 )
	"vmulpd            %%ymm7,  %%ymm13, %%ymm1  \n\t" // scale by alpha1, ymm1 = ymm7( alpha1 ) * ymm13( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 0:3, 0 )
	"vmovapd    1 * 32(%%rcx),  %%ymm3           \n\t" // ymm3 = c_list[0]( 4:7, 0 )
	"vmulpd            %%ymm7,  %%ymm12, %%ymm2  \n\t" // scale by alpha1, ymm2 = ymm7( alpha1 ) * ymm12( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 4:7, 0 )
	"addq              %%rdi,   %%rcx            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rcx),  %%ymm0           \n\t" // ymm0 = c_list[0]( 0:3, 0 )
	"vmulpd            %%ymm7,  %%ymm15, %%ymm1  \n\t" // scale by alpha1, ymm1 = ymm7( alpha1 ) * ymm15( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 0:3, 0 )
	"vmovapd    1 * 32(%%rcx),  %%ymm3           \n\t" // ymm3 = c_list[0]( 4:7, 0 )
	"vmulpd            %%ymm7,  %%ymm14, %%ymm2  \n\t" // scale by alpha1, ymm2 = ymm7( alpha1 ) * ymm14( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rcx)    \n\t" // and store back to memory: c_list[0]( 4:7, 0 )
    "                                            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rdx),  %%ymm0           \n\t" // ymm0 = c_list[1]( 0:3, 0 )
	"vmulpd            %%ymm6,  %%ymm9,  %%ymm1  \n\t" // scale by alpha2, ymm1 = ymm6( alpha2 ) * ymm9( ab0_3:0 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 0:3, 0 )
	"vmovapd    1 * 32(%%rdx),  %%ymm3           \n\t" // ymm3 = c_list[1]( 4:7, 0 )
	"vmulpd            %%ymm6,  %%ymm8,  %%ymm2  \n\t" // scale by alpha2, ymm2 = ymm6( alpha2 ) * ymm8( ab4_7:0 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 4:7, 0 )
	"addq              %%rdi,   %%rdx            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rdx),  %%ymm0           \n\t" // ymm0 = c_list[1]( 0:3, 0 )
	"vmulpd            %%ymm6,  %%ymm11, %%ymm1  \n\t" // scale by alpha2, ymm1 = ymm6( alpha2 ) * ymm11( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 0:3, 0 )
	"vmovapd    1 * 32(%%rdx),  %%ymm3           \n\t" // ymm3 = c_list[1]( 4:7, 0 )
	"vmulpd            %%ymm6,  %%ymm10, %%ymm2  \n\t" // scale by alpha2, ymm2 = ymm6( alpha2 ) * ymm10( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 4:7, 0 )
	"addq              %%rdi,   %%rdx            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rdx),  %%ymm0           \n\t" // ymm0 = c_list[1]( 0:3, 0 )
	"vmulpd            %%ymm6,  %%ymm13, %%ymm1  \n\t" // scale by alpha2, ymm1 = ymm6( alpha2 ) * ymm13( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 0:3, 0 )
	"vmovapd    1 * 32(%%rdx),  %%ymm3           \n\t" // ymm3 = c_list[1]( 4:7, 0 )
	"vmulpd            %%ymm6,  %%ymm12, %%ymm2  \n\t" // scale by alpha2, ymm2 = ymm6( alpha2 ) * ymm12( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 4:7, 0 )
	"addq              %%rdi,   %%rdx            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%rdx),  %%ymm0           \n\t" // ymm0 = c_list[1]( 0:3, 0 )
	"vmulpd            %%ymm6,  %%ymm15, %%ymm1  \n\t" // scale by alpha2, ymm1 = ymm6( alpha2 ) * ymm15( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 0:3, 0 )
	"vmovapd    1 * 32(%%rdx),  %%ymm3           \n\t" // ymm3 = c_list[1]( 4:7, 0 )
	"vmulpd            %%ymm6,  %%ymm14, %%ymm2  \n\t" // scale by alpha2, ymm2 = ymm6( alpha2 ) * ymm14( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%rdx)    \n\t" // and store back to memory: c_list[1]( 4:7, 0 )
    "                                            \n\t"
    "                                            \n\t"
    "                                            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r10),  %%ymm0           \n\t" // ymm0 = c_list[2]( 0:3, 0 )
	"vmulpd            %%ymm5,  %%ymm9,  %%ymm1  \n\t" // scale by alpha3, ymm1 = ymm5( alpha3 ) * ymm9( ab0_3:0 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 0:3, 0 )
	"vmovapd    1 * 32(%%r10),  %%ymm3           \n\t" // ymm3 = c_list[2]( 4:7, 0 )
	"vmulpd            %%ymm5,  %%ymm8,  %%ymm2  \n\t" // scale by alpha3, ymm2 = ymm5( alpha3 ) * ymm8( ab4_7:0 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 4:7, 0 )
	"addq              %%rdi,   %%r10            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r10),  %%ymm0           \n\t" // ymm0 = c_list[2]( 0:3, 0 )
	"vmulpd            %%ymm5,  %%ymm11, %%ymm1  \n\t" // scale by alpha3, ymm1 = ymm5( alpha3 ) * ymm11( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 0:3, 0 )
	"vmovapd    1 * 32(%%r10),  %%ymm3           \n\t" // ymm3 = c_list[2]( 4:7, 0 )
	"vmulpd            %%ymm5,  %%ymm10, %%ymm2  \n\t" // scale by alpha3, ymm2 = ymm5( alpha3 ) * ymm10( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 4:7, 0 )
	"addq              %%rdi,   %%r10            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r10),  %%ymm0           \n\t" // ymm0 = c_list[2]( 0:3, 0 )
	"vmulpd            %%ymm5,  %%ymm13, %%ymm1  \n\t" // scale by alpha3, ymm1 = ymm5( alpha3 ) * ymm13( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 0:3, 0 )
	"vmovapd    1 * 32(%%r10),  %%ymm3           \n\t" // ymm3 = c_list[2]( 4:7, 0 )
	"vmulpd            %%ymm5,  %%ymm12, %%ymm2  \n\t" // scale by alpha3, ymm2 = ymm5( alpha3 ) * ymm12( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 4:7, 0 )
	"addq              %%rdi,   %%r10            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r10),  %%ymm0           \n\t" // ymm0 = c_list[2]( 0:3, 0 )
	"vmulpd            %%ymm5,  %%ymm15, %%ymm1  \n\t" // scale by alpha3, ymm1 = ymm5( alpha3 ) * ymm15( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 0:3, 0 )
	"vmovapd    1 * 32(%%r10),  %%ymm3           \n\t" // ymm3 = c_list[2]( 4:7, 0 )
	"vmulpd            %%ymm5,  %%ymm14, %%ymm2  \n\t" // scale by alpha3, ymm2 = ymm5( alpha3 ) * ymm14( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r10)    \n\t" // and store back to memory: c_list[2]( 4:7, 0 )
    "                                            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r14),  %%ymm0           \n\t" // ymm0 = c_list[3]( 0:3, 0 )
	"vmulpd            %%ymm4,  %%ymm9,  %%ymm1  \n\t" // scale by alpha4, ymm1 = ymm4( alpha4 ) * ymm9( ab0_3:0 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 0:3, 0 )
	"vmovapd    1 * 32(%%r14),  %%ymm3           \n\t" // ymm3 = c_list[3]( 4:7, 0 )
	"vmulpd            %%ymm4,  %%ymm8,  %%ymm2  \n\t" // scale by alpha4, ymm2 = ymm4( alpha4 ) * ymm8( ab4_7:0 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 4:7, 0 )
	"addq              %%rdi,   %%r14            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r14),  %%ymm0           \n\t" // ymm0 = c_list[3]( 0:3, 0 )
	"vmulpd            %%ymm4,  %%ymm11, %%ymm1  \n\t" // scale by alpha4, ymm1 = ymm4( alpha4 ) * ymm11( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 0:3, 0 )
	"vmovapd    1 * 32(%%r14),  %%ymm3           \n\t" // ymm3 = c_list[3]( 4:7, 0 )
	"vmulpd            %%ymm4,  %%ymm10, %%ymm2  \n\t" // scale by alpha4, ymm2 = ymm4( alpha4 ) * ymm10( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 4:7, 0 )
	"addq              %%rdi,   %%r14            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r14),  %%ymm0           \n\t" // ymm0 = c_list[3]( 0:3, 0 )
	"vmulpd            %%ymm4,  %%ymm13, %%ymm1  \n\t" // scale by alpha4, ymm1 = ymm4( alpha4 ) * ymm13( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 0:3, 0 )
	"vmovapd    1 * 32(%%r14),  %%ymm3           \n\t" // ymm3 = c_list[3]( 4:7, 0 )
	"vmulpd            %%ymm4,  %%ymm12, %%ymm2  \n\t" // scale by alpha4, ymm2 = ymm4( alpha4 ) * ymm12( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 4:7, 0 )
	"addq              %%rdi,   %%r14            \n\t"
    "                                            \n\t"
	"vmovapd    0 * 32(%%r14),  %%ymm0           \n\t" // ymm0 = c_list[3]( 0:3, 0 )
	"vmulpd            %%ymm4,  %%ymm15, %%ymm1  \n\t" // scale by alpha4, ymm1 = ymm4( alpha4 ) * ymm15( ab0_3:1 )
	"vaddpd            %%ymm1,  %%ymm0,  %%ymm1  \n\t" // ymm1 = ymm0 + ymm1
	"vmovapd           %%ymm1,  0 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 0:3, 0 )
	"vmovapd    1 * 32(%%r14),  %%ymm3           \n\t" // ymm3 = c_list[3]( 4:7, 0 )
	"vmulpd            %%ymm4,  %%ymm14, %%ymm2  \n\t" // scale by alpha4, ymm2 = ymm4( alpha4 ) * ymm14( ab4_7:1 )
	"vaddpd            %%ymm2,  %%ymm3,  %%ymm2  \n\t" // ymm2 = ymm3 + ymm2
	"vmovapd           %%ymm2,  1 * 32(%%r14)    \n\t" // and store back to memory: c_list[3]( 4:7, 0 )
    "                                            \n\t"
    "                                            \n\t"
    "                                            \n\t"
    "                                            \n\t"
    ".DDONE:                                    \n\t"
    "                                            \n\t"
    : // output operands (none)
    : // input operands
      [k_iter]     "m" (k_iter),      // 0
      [k_left]     "m" (k_left),      // 1
      [a]          "m" (a),           // 2
      [b]          "m" (b),           // 3
      [b_next]     "m" (b_next),      // 4
      [rs_c]       "m" (rs_c),        // 5
      [cs_c]       "m" (cs_c),        // 6
      [c0]         "m" (c0)           // 7
      [c1]         "m" (c1)           // 8
      [c2]         "m" (c2)           // 9
      [c3]         "m" (c3)           // 10
      [coeff0]     "m" (coeff0)       // 11
      [coeff1]     "m" (coeff1)       // 12
      [coeff2]     "m" (coeff2)       // 13
      [coeff3]     "m" (coeff3)       // 14
    : // register clobber list
      "rax", "rbx", "rcx", "rdx", "rsi", "rdi",
      "r8", "r9", "r10", "r11", "r12", "r13", "r14", "r15",
      "xmm0", "xmm1", "xmm2", "xmm3",
      "xmm4", "xmm5", "xmm6", "xmm7",
      "xmm8", "xmm9", "xmm10", "xmm11",
      "xmm12", "xmm13", "xmm14", "xmm15",
      "memory"
    );
}
